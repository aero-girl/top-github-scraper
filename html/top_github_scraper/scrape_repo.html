<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>top_github_scraper.scrape_repo API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>top_github_scraper.scrape_repo</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import json
import os
from pathlib import Path
from typing import List

import pandas as pd
import requests
from dotenv import load_dotenv
from rich import print
from rich.progress import track
from tqdm import tqdm
from top_github_scraper.utils import ScrapeGithubUrl, UserProfileGetter, isnotebook
import logging

load_dotenv()

USERNAME = os.getenv(&#34;GITHUB_USERNAME&#34;)
TOKEN = os.getenv(&#34;GITHUB_TOKEN&#34;)

class RepoScraper:
    &#34;&#34;&#34;Scrape information of repos and the
    contributors of those repositories&#34;&#34;&#34;

    def __init__(self, repo_urls: list, max_n_top_contributors: int):
        self.repo_urls = repo_urls
        self.max_n_top_contributors = max_n_top_contributors

    def get_all_top_repo_information(self):
        top_repo_infos = []

        if isnotebook():
            for repo_url in tqdm(
            self.repo_urls, desc=&#34;Scraping top GitHub repositories...&#34;
            ):
                top_repo_infos.append(self._get_repo_information(repo_url))
        else:
            for repo_url in track(
                self.repo_urls, description=&#34;Scraping top GitHub repositories...&#34;
            ):
                top_repo_infos.append(self._get_repo_information(repo_url))

        return top_repo_infos

    def _get_repo_information(self, repo_url: str):
        repo_info_url = f&#34;https://api.github.com/repos{repo_url}&#34;
        repo_info = requests.get(repo_info_url, auth=(USERNAME, TOKEN)).json()
        info_to_scrape = [&#34;stargazers_count&#34;, &#34;forks_count&#34;]
        repo_important_info = {}
        for info in info_to_scrape:
            repo_important_info[info] = repo_info[info]

        repo_important_info[
            &#34;contributors&#34;
        ] = self._get_contributor_repo_of_one_repo(repo_url)

        return repo_important_info

    def _get_contributor_repo_of_one_repo(self, repo_url: str):

        # https://api.github.com/repos/josephmisiti/awesome-machine-learning/contributors
        contributor_url = (
            f&#34;https://api.github.com/repos{repo_url}/contributors&#34;
        )
        contributor_page = requests.get(
            contributor_url, auth=(USERNAME, TOKEN)
        ).json()

        contributors_info = {&#34;login&#34;: [], &#34;url&#34;: [], &#34;contributions&#34;: []}

        max_n_top_contributors = self._find_max_n_top_contributors(
            num_contributors=len(contributor_page)
        )
        n_top_contributor = 0

        while n_top_contributor &lt; max_n_top_contributors:
            contributor = contributor_page[n_top_contributor]

            self._get_contributor_general_info(contributors_info, contributor)
            n_top_contributor += 1

        return contributors_info

    @staticmethod
    def _get_contributor_general_info(
        contributors_info: List[dict], contributor: dict
    ):

        contributors_info[&#34;login&#34;].append(contributor[&#34;login&#34;])
        contributors_info[&#34;url&#34;].append(contributor[&#34;url&#34;])
        contributors_info[&#34;contributions&#34;].append(contributor[&#34;contributions&#34;])

    def _find_max_n_top_contributors(self, num_contributors: int):
        if num_contributors &gt; self.max_n_top_contributors:
            return self.max_n_top_contributors
        else:
            return num_contributors


class DataProcessor:
    def __init__(self, data: list):
        self.data = data

    def process(self) -&gt; pd.DataFrame:

        repos = [self.process_one_repo(repo) for repo in self.data]
        return pd.concat(repos).reset_index(drop=True)

    def process_one_repo(self, repo_info: dict):
        contributors_info = repo_info[&#34;contributors&#34;]
        contributors_info = pd.DataFrame(contributors_info)

        repo_stats = self.get_repo_stats(repo_info)

        for col_name, val in repo_stats.items():
            contributors_info[col_name] = val

        return contributors_info

    @staticmethod
    def get_repo_stats(repo_info: dict):
        repo_stats_list = [
            &#34;stargazers_count&#34;,
            &#34;forks_count&#34;,
            &#34;created_at&#34;,
            &#34;updated_at&#34;,
        ]
        return {
            key: val
            for key, val in repo_info.items()
            if key in repo_stats_list
        }


def get_top_repo_urls(
    keyword: str,
    sort_by: str=&#39;best_match&#39;, 
    save_directory: str=&#34;.&#34;,
    start_page: int = 1,
    stop_page: int = 10,
):
    &#34;&#34;&#34;Get the URLs of the repositories pop up when searching for a specific
    keyword on GitHub.

    Parameters
    ----------
    keyword : str
        Keyword to search for (.i.e, machine learning)
    sort_by: str 
        sort by best match or most stars, by default &#39;best_match&#39;, which will sort by best match. 
        Use &#39;stars&#39; to sort by most stars.
    save_directory: str, optional 
        directory to save the output file, by default &#34;.&#34;
    start_page : int, optional
        page number to start scraping from, by default 1
    stop_page : int, optional
        page number of the last page to scrape, by default 10
    &#34;&#34;&#34;
    try: 
        Path(save_directory).mkdir(parents=True, exist_ok=True)
        full_path = f&#39;{save_directory}/top_repo_urls_{keyword}_{sort_by}_{start_page}_{stop_page}.json&#39;
        repo_urls = ScrapeGithubUrl(
            keyword, &#39;Repositories&#39;, sort_by, start_page, stop_page
        ).scrape_top_repo_url_multiple_pages()

        with open(full_path, &#34;w&#34;) as outfile:
            json.dump(repo_urls, outfile)
        return repo_urls
    except Exception as e:
        print(e)
        logging.error(&#34;&#34;&#34;You might ran out of rate limit. Are you an authenticated user? If you ran out of rate limit while requesting as an authenticated user, 
        either decrease the number of pages to scrape or to wait until more requests are available.&#34;&#34;&#34;)


def get_top_repos(
    keyword: int,
    sort_by: str=&#39;best_match&#39;,
    save_directory: str=&#34;.&#34;,
    max_n_top_contributors: int = 10,
    start_page: int = 1,
    stop_page: int = 10,
):
    &#34;&#34;&#34;Get the information of the repositories pop up when searching for a specific
    keyword on GitHub.

    Parameters
    ----------
    keyword : str
        Keyword to search for (.i.e, machine learning)
    sort_by: str 
        sort by best match or most stars, by default &#39;best_match&#39;, which will sort by best match. 
        Use &#39;stars&#39; to sort by most stars.
    max_n_top_contributors: int
        number of top contributors in each repository to scrape from, by default 10
    start_page : int, optional
        page number to start scraping from, by default 1
    stop_page : int, optional
        page number of the last page to scrape, by default 10
    save_directory: str, optional 
        directory to save the output file, by default &#34;.&#34;
    &#34;&#34;&#34;
    try:
        full_url_save_path = (
            f&#34;{save_directory}/top_repo_urls_{keyword}_{sort_by}_{start_page}_{stop_page}.json&#34;
        )
        repo_save_path = f&#34;{save_directory}/top_repo_info_{keyword}_{sort_by}_{start_page}_{stop_page}.json&#34;

        if not Path(full_url_save_path).exists():
            get_top_repo_urls(keyword=keyword, sort_by=sort_by, save_directory=save_directory, start_page=start_page, stop_page=stop_page)
        with open(full_url_save_path, &#34;r&#34;) as infile:
            repo_urls = json.load(infile)
            top_repos = RepoScraper(
                repo_urls, max_n_top_contributors
            ).get_all_top_repo_information()

        with open(repo_save_path, &#34;w&#34;) as outfile:
            json.dump(top_repos, outfile)
        return top_repos

    except Exception as e:  
        print(e)
        logging.error(&#34;&#34;&#34;You might ran out of rate limit. Are you an authenticated user? If you ran out of rate limit while requesting as an authenticated user, 
        either decrease the number of pages to scrape or to wait until more requests are available.&#34;&#34;&#34;)

def get_top_contributors(
    keyword: int,
    sort_by: str=&#39;best_match&#39;, 
    max_n_top_contributors: int = 10,
    start_page: int = 1,
    stop_page: int = 10,
    get_user_info_only: bool=True, 
    save_directory: str=&#34;.&#34;,
):
    &#34;&#34;&#34;
    Get the information of the owners and contributors of the repositories pop up when searching for a specific
    keyword on GitHub.
    Parameters
    ----------
    keyword : str
        Keyword to search for (.i.e, machine learning)
    sort_by: str 
        sort by best match or most stars, by default &#39;best_match&#39;, which will sort by best match. 
        Use &#39;stars&#39; to sort by most stars.
    max_n_top_contributors: int
        number of top contributors in each repository to scrape from, by default 10
    start_page : int, optional
        page number to start scraping from, by default 1
    stop_page : int, optional
        page number of the last page to scrape, by default 10
    get_user_info_only: bool, optional
        whether to get the information of only contributors or to get the information of both contributors 
        and repositories contributors were scraped from, by default True, which means to get only contributors&#39; information
    save_directory: str, optional 
        directory to save the output file, by default &#34;.&#34;
    url_save_path : str, optional
        where to save the output file of URLs, by default &#34;top_repo_urls&#34;
    repo_save_path : str, optional
        where to save the output file of repositories&#39; information, by default &#34;top_repo_info&#34;
    user_save_path : str, optional
        where to save the output file of users&#39; profiles, by default &#34;top_contributor_info&#34;
    &#34;&#34;&#34;

    full_repo_save_path = (
        f&#34;{save_directory}/top_repo_info_{keyword}_{sort_by}_{start_page}_{stop_page}.json&#34;
    )
    user_save_path = f&#34;{save_directory}/top_contributor_info_{keyword}_{sort_by}_{start_page}_{stop_page}.csv&#34;
    if not Path(full_repo_save_path).exists():
        get_top_repos(
            keyword=keyword,
            sort_by=sort_by,
            max_n_top_contributors=max_n_top_contributors,
            start_page=start_page,
            stop_page=stop_page,
            save_directory=save_directory
        )
    with open(full_repo_save_path, &#34;r&#34;) as infile:
        repo_info = json.load(infile)
        repo_info = DataProcessor(repo_info).process()
        urls = repo_info[&#39;url&#39;]
        top_users = UserProfileGetter(urls).get_all_user_profiles()
        if get_user_info_only:
            top_users.to_csv(user_save_path)
            return top_users
        else:
            repo_and_top_users = pd.concat([repo_info, top_users], axis=1)
            repo_and_top_users = repo_and_top_users.loc[:,~repo_and_top_users.columns.duplicated()]
            repo_and_top_users.to_csv(user_save_path)
            return repo_and_top_users</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="top_github_scraper.scrape_repo.get_top_contributors"><code class="name flex">
<span>def <span class="ident">get_top_contributors</span></span>(<span>keyword: int, sort_by: str = 'best_match', max_n_top_contributors: int = 10, start_page: int = 1, stop_page: int = 10, get_user_info_only: bool = True, save_directory: str = '.')</span>
</code></dt>
<dd>
<div class="desc"><p>Get the information of the owners and contributors of the repositories pop up when searching for a specific
keyword on GitHub.
Parameters</p>
<hr>
<dl>
<dt><strong><code>keyword</code></strong> :&ensp;<code>str</code></dt>
<dd>Keyword to search for (.i.e, machine learning)</dd>
<dt><strong><code>sort_by</code></strong> :&ensp;<code>str </code></dt>
<dd>sort by best match or most stars, by default 'best_match', which will sort by best match.
Use 'stars' to sort by most stars.</dd>
<dt><strong><code>max_n_top_contributors</code></strong> :&ensp;<code>int</code></dt>
<dd>number of top contributors in each repository to scrape from, by default 10</dd>
<dt><strong><code>start_page</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>page number to start scraping from, by default 1</dd>
<dt><strong><code>stop_page</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>page number of the last page to scrape, by default 10</dd>
<dt><strong><code>get_user_info_only</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>whether to get the information of only contributors or to get the information of both contributors
and repositories contributors were scraped from, by default True, which means to get only contributors' information</dd>
<dt><strong><code>save_directory</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>directory to save the output file, by default "."</dd>
<dt><strong><code>url_save_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>where to save the output file of URLs, by default "top_repo_urls"</dd>
<dt><strong><code>repo_save_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>where to save the output file of repositories' information, by default "top_repo_info"</dd>
<dt><strong><code>user_save_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>where to save the output file of users' profiles, by default "top_contributor_info"</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_top_contributors(
    keyword: int,
    sort_by: str=&#39;best_match&#39;, 
    max_n_top_contributors: int = 10,
    start_page: int = 1,
    stop_page: int = 10,
    get_user_info_only: bool=True, 
    save_directory: str=&#34;.&#34;,
):
    &#34;&#34;&#34;
    Get the information of the owners and contributors of the repositories pop up when searching for a specific
    keyword on GitHub.
    Parameters
    ----------
    keyword : str
        Keyword to search for (.i.e, machine learning)
    sort_by: str 
        sort by best match or most stars, by default &#39;best_match&#39;, which will sort by best match. 
        Use &#39;stars&#39; to sort by most stars.
    max_n_top_contributors: int
        number of top contributors in each repository to scrape from, by default 10
    start_page : int, optional
        page number to start scraping from, by default 1
    stop_page : int, optional
        page number of the last page to scrape, by default 10
    get_user_info_only: bool, optional
        whether to get the information of only contributors or to get the information of both contributors 
        and repositories contributors were scraped from, by default True, which means to get only contributors&#39; information
    save_directory: str, optional 
        directory to save the output file, by default &#34;.&#34;
    url_save_path : str, optional
        where to save the output file of URLs, by default &#34;top_repo_urls&#34;
    repo_save_path : str, optional
        where to save the output file of repositories&#39; information, by default &#34;top_repo_info&#34;
    user_save_path : str, optional
        where to save the output file of users&#39; profiles, by default &#34;top_contributor_info&#34;
    &#34;&#34;&#34;

    full_repo_save_path = (
        f&#34;{save_directory}/top_repo_info_{keyword}_{sort_by}_{start_page}_{stop_page}.json&#34;
    )
    user_save_path = f&#34;{save_directory}/top_contributor_info_{keyword}_{sort_by}_{start_page}_{stop_page}.csv&#34;
    if not Path(full_repo_save_path).exists():
        get_top_repos(
            keyword=keyword,
            sort_by=sort_by,
            max_n_top_contributors=max_n_top_contributors,
            start_page=start_page,
            stop_page=stop_page,
            save_directory=save_directory
        )
    with open(full_repo_save_path, &#34;r&#34;) as infile:
        repo_info = json.load(infile)
        repo_info = DataProcessor(repo_info).process()
        urls = repo_info[&#39;url&#39;]
        top_users = UserProfileGetter(urls).get_all_user_profiles()
        if get_user_info_only:
            top_users.to_csv(user_save_path)
            return top_users
        else:
            repo_and_top_users = pd.concat([repo_info, top_users], axis=1)
            repo_and_top_users = repo_and_top_users.loc[:,~repo_and_top_users.columns.duplicated()]
            repo_and_top_users.to_csv(user_save_path)
            return repo_and_top_users</code></pre>
</details>
</dd>
<dt id="top_github_scraper.scrape_repo.get_top_repo_urls"><code class="name flex">
<span>def <span class="ident">get_top_repo_urls</span></span>(<span>keyword: str, sort_by: str = 'best_match', save_directory: str = '.', start_page: int = 1, stop_page: int = 10)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the URLs of the repositories pop up when searching for a specific
keyword on GitHub.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>keyword</code></strong> :&ensp;<code>str</code></dt>
<dd>Keyword to search for (.i.e, machine learning)</dd>
<dt><strong><code>sort_by</code></strong> :&ensp;<code>str </code></dt>
<dd>sort by best match or most stars, by default 'best_match', which will sort by best match.
Use 'stars' to sort by most stars.</dd>
<dt><strong><code>save_directory</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>directory to save the output file, by default "."</dd>
<dt><strong><code>start_page</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>page number to start scraping from, by default 1</dd>
<dt><strong><code>stop_page</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>page number of the last page to scrape, by default 10</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_top_repo_urls(
    keyword: str,
    sort_by: str=&#39;best_match&#39;, 
    save_directory: str=&#34;.&#34;,
    start_page: int = 1,
    stop_page: int = 10,
):
    &#34;&#34;&#34;Get the URLs of the repositories pop up when searching for a specific
    keyword on GitHub.

    Parameters
    ----------
    keyword : str
        Keyword to search for (.i.e, machine learning)
    sort_by: str 
        sort by best match or most stars, by default &#39;best_match&#39;, which will sort by best match. 
        Use &#39;stars&#39; to sort by most stars.
    save_directory: str, optional 
        directory to save the output file, by default &#34;.&#34;
    start_page : int, optional
        page number to start scraping from, by default 1
    stop_page : int, optional
        page number of the last page to scrape, by default 10
    &#34;&#34;&#34;
    try: 
        Path(save_directory).mkdir(parents=True, exist_ok=True)
        full_path = f&#39;{save_directory}/top_repo_urls_{keyword}_{sort_by}_{start_page}_{stop_page}.json&#39;
        repo_urls = ScrapeGithubUrl(
            keyword, &#39;Repositories&#39;, sort_by, start_page, stop_page
        ).scrape_top_repo_url_multiple_pages()

        with open(full_path, &#34;w&#34;) as outfile:
            json.dump(repo_urls, outfile)
        return repo_urls
    except Exception as e:
        print(e)
        logging.error(&#34;&#34;&#34;You might ran out of rate limit. Are you an authenticated user? If you ran out of rate limit while requesting as an authenticated user, 
        either decrease the number of pages to scrape or to wait until more requests are available.&#34;&#34;&#34;)</code></pre>
</details>
</dd>
<dt id="top_github_scraper.scrape_repo.get_top_repos"><code class="name flex">
<span>def <span class="ident">get_top_repos</span></span>(<span>keyword: int, sort_by: str = 'best_match', save_directory: str = '.', max_n_top_contributors: int = 10, start_page: int = 1, stop_page: int = 10)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the information of the repositories pop up when searching for a specific
keyword on GitHub.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>keyword</code></strong> :&ensp;<code>str</code></dt>
<dd>Keyword to search for (.i.e, machine learning)</dd>
<dt><strong><code>sort_by</code></strong> :&ensp;<code>str </code></dt>
<dd>sort by best match or most stars, by default 'best_match', which will sort by best match.
Use 'stars' to sort by most stars.</dd>
<dt><strong><code>max_n_top_contributors</code></strong> :&ensp;<code>int</code></dt>
<dd>number of top contributors in each repository to scrape from, by default 10</dd>
<dt><strong><code>start_page</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>page number to start scraping from, by default 1</dd>
<dt><strong><code>stop_page</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>page number of the last page to scrape, by default 10</dd>
<dt><strong><code>save_directory</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>directory to save the output file, by default "."</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_top_repos(
    keyword: int,
    sort_by: str=&#39;best_match&#39;,
    save_directory: str=&#34;.&#34;,
    max_n_top_contributors: int = 10,
    start_page: int = 1,
    stop_page: int = 10,
):
    &#34;&#34;&#34;Get the information of the repositories pop up when searching for a specific
    keyword on GitHub.

    Parameters
    ----------
    keyword : str
        Keyword to search for (.i.e, machine learning)
    sort_by: str 
        sort by best match or most stars, by default &#39;best_match&#39;, which will sort by best match. 
        Use &#39;stars&#39; to sort by most stars.
    max_n_top_contributors: int
        number of top contributors in each repository to scrape from, by default 10
    start_page : int, optional
        page number to start scraping from, by default 1
    stop_page : int, optional
        page number of the last page to scrape, by default 10
    save_directory: str, optional 
        directory to save the output file, by default &#34;.&#34;
    &#34;&#34;&#34;
    try:
        full_url_save_path = (
            f&#34;{save_directory}/top_repo_urls_{keyword}_{sort_by}_{start_page}_{stop_page}.json&#34;
        )
        repo_save_path = f&#34;{save_directory}/top_repo_info_{keyword}_{sort_by}_{start_page}_{stop_page}.json&#34;

        if not Path(full_url_save_path).exists():
            get_top_repo_urls(keyword=keyword, sort_by=sort_by, save_directory=save_directory, start_page=start_page, stop_page=stop_page)
        with open(full_url_save_path, &#34;r&#34;) as infile:
            repo_urls = json.load(infile)
            top_repos = RepoScraper(
                repo_urls, max_n_top_contributors
            ).get_all_top_repo_information()

        with open(repo_save_path, &#34;w&#34;) as outfile:
            json.dump(top_repos, outfile)
        return top_repos

    except Exception as e:  
        print(e)
        logging.error(&#34;&#34;&#34;You might ran out of rate limit. Are you an authenticated user? If you ran out of rate limit while requesting as an authenticated user, 
        either decrease the number of pages to scrape or to wait until more requests are available.&#34;&#34;&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="top_github_scraper.scrape_repo.DataProcessor"><code class="flex name class">
<span>class <span class="ident">DataProcessor</span></span>
<span>(</span><span>data: list)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataProcessor:
    def __init__(self, data: list):
        self.data = data

    def process(self) -&gt; pd.DataFrame:

        repos = [self.process_one_repo(repo) for repo in self.data]
        return pd.concat(repos).reset_index(drop=True)

    def process_one_repo(self, repo_info: dict):
        contributors_info = repo_info[&#34;contributors&#34;]
        contributors_info = pd.DataFrame(contributors_info)

        repo_stats = self.get_repo_stats(repo_info)

        for col_name, val in repo_stats.items():
            contributors_info[col_name] = val

        return contributors_info

    @staticmethod
    def get_repo_stats(repo_info: dict):
        repo_stats_list = [
            &#34;stargazers_count&#34;,
            &#34;forks_count&#34;,
            &#34;created_at&#34;,
            &#34;updated_at&#34;,
        ]
        return {
            key: val
            for key, val in repo_info.items()
            if key in repo_stats_list
        }</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="top_github_scraper.scrape_repo.DataProcessor.get_repo_stats"><code class="name flex">
<span>def <span class="ident">get_repo_stats</span></span>(<span>repo_info: dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_repo_stats(repo_info: dict):
    repo_stats_list = [
        &#34;stargazers_count&#34;,
        &#34;forks_count&#34;,
        &#34;created_at&#34;,
        &#34;updated_at&#34;,
    ]
    return {
        key: val
        for key, val in repo_info.items()
        if key in repo_stats_list
    }</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="top_github_scraper.scrape_repo.DataProcessor.process"><code class="name flex">
<span>def <span class="ident">process</span></span>(<span>self) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process(self) -&gt; pd.DataFrame:

    repos = [self.process_one_repo(repo) for repo in self.data]
    return pd.concat(repos).reset_index(drop=True)</code></pre>
</details>
</dd>
<dt id="top_github_scraper.scrape_repo.DataProcessor.process_one_repo"><code class="name flex">
<span>def <span class="ident">process_one_repo</span></span>(<span>self, repo_info: dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_one_repo(self, repo_info: dict):
    contributors_info = repo_info[&#34;contributors&#34;]
    contributors_info = pd.DataFrame(contributors_info)

    repo_stats = self.get_repo_stats(repo_info)

    for col_name, val in repo_stats.items():
        contributors_info[col_name] = val

    return contributors_info</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="top_github_scraper.scrape_repo.RepoScraper"><code class="flex name class">
<span>class <span class="ident">RepoScraper</span></span>
<span>(</span><span>repo_urls: list, max_n_top_contributors: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Scrape information of repos and the
contributors of those repositories</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RepoScraper:
    &#34;&#34;&#34;Scrape information of repos and the
    contributors of those repositories&#34;&#34;&#34;

    def __init__(self, repo_urls: list, max_n_top_contributors: int):
        self.repo_urls = repo_urls
        self.max_n_top_contributors = max_n_top_contributors

    def get_all_top_repo_information(self):
        top_repo_infos = []

        if isnotebook():
            for repo_url in tqdm(
            self.repo_urls, desc=&#34;Scraping top GitHub repositories...&#34;
            ):
                top_repo_infos.append(self._get_repo_information(repo_url))
        else:
            for repo_url in track(
                self.repo_urls, description=&#34;Scraping top GitHub repositories...&#34;
            ):
                top_repo_infos.append(self._get_repo_information(repo_url))

        return top_repo_infos

    def _get_repo_information(self, repo_url: str):
        repo_info_url = f&#34;https://api.github.com/repos{repo_url}&#34;
        repo_info = requests.get(repo_info_url, auth=(USERNAME, TOKEN)).json()
        info_to_scrape = [&#34;stargazers_count&#34;, &#34;forks_count&#34;]
        repo_important_info = {}
        for info in info_to_scrape:
            repo_important_info[info] = repo_info[info]

        repo_important_info[
            &#34;contributors&#34;
        ] = self._get_contributor_repo_of_one_repo(repo_url)

        return repo_important_info

    def _get_contributor_repo_of_one_repo(self, repo_url: str):

        # https://api.github.com/repos/josephmisiti/awesome-machine-learning/contributors
        contributor_url = (
            f&#34;https://api.github.com/repos{repo_url}/contributors&#34;
        )
        contributor_page = requests.get(
            contributor_url, auth=(USERNAME, TOKEN)
        ).json()

        contributors_info = {&#34;login&#34;: [], &#34;url&#34;: [], &#34;contributions&#34;: []}

        max_n_top_contributors = self._find_max_n_top_contributors(
            num_contributors=len(contributor_page)
        )
        n_top_contributor = 0

        while n_top_contributor &lt; max_n_top_contributors:
            contributor = contributor_page[n_top_contributor]

            self._get_contributor_general_info(contributors_info, contributor)
            n_top_contributor += 1

        return contributors_info

    @staticmethod
    def _get_contributor_general_info(
        contributors_info: List[dict], contributor: dict
    ):

        contributors_info[&#34;login&#34;].append(contributor[&#34;login&#34;])
        contributors_info[&#34;url&#34;].append(contributor[&#34;url&#34;])
        contributors_info[&#34;contributions&#34;].append(contributor[&#34;contributions&#34;])

    def _find_max_n_top_contributors(self, num_contributors: int):
        if num_contributors &gt; self.max_n_top_contributors:
            return self.max_n_top_contributors
        else:
            return num_contributors</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="top_github_scraper.scrape_repo.RepoScraper.get_all_top_repo_information"><code class="name flex">
<span>def <span class="ident">get_all_top_repo_information</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_all_top_repo_information(self):
    top_repo_infos = []

    if isnotebook():
        for repo_url in tqdm(
        self.repo_urls, desc=&#34;Scraping top GitHub repositories...&#34;
        ):
            top_repo_infos.append(self._get_repo_information(repo_url))
    else:
        for repo_url in track(
            self.repo_urls, description=&#34;Scraping top GitHub repositories...&#34;
        ):
            top_repo_infos.append(self._get_repo_information(repo_url))

    return top_repo_infos</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="top_github_scraper" href="index.html">top_github_scraper</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="top_github_scraper.scrape_repo.get_top_contributors" href="#top_github_scraper.scrape_repo.get_top_contributors">get_top_contributors</a></code></li>
<li><code><a title="top_github_scraper.scrape_repo.get_top_repo_urls" href="#top_github_scraper.scrape_repo.get_top_repo_urls">get_top_repo_urls</a></code></li>
<li><code><a title="top_github_scraper.scrape_repo.get_top_repos" href="#top_github_scraper.scrape_repo.get_top_repos">get_top_repos</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="top_github_scraper.scrape_repo.DataProcessor" href="#top_github_scraper.scrape_repo.DataProcessor">DataProcessor</a></code></h4>
<ul class="">
<li><code><a title="top_github_scraper.scrape_repo.DataProcessor.get_repo_stats" href="#top_github_scraper.scrape_repo.DataProcessor.get_repo_stats">get_repo_stats</a></code></li>
<li><code><a title="top_github_scraper.scrape_repo.DataProcessor.process" href="#top_github_scraper.scrape_repo.DataProcessor.process">process</a></code></li>
<li><code><a title="top_github_scraper.scrape_repo.DataProcessor.process_one_repo" href="#top_github_scraper.scrape_repo.DataProcessor.process_one_repo">process_one_repo</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="top_github_scraper.scrape_repo.RepoScraper" href="#top_github_scraper.scrape_repo.RepoScraper">RepoScraper</a></code></h4>
<ul class="">
<li><code><a title="top_github_scraper.scrape_repo.RepoScraper.get_all_top_repo_information" href="#top_github_scraper.scrape_repo.RepoScraper.get_all_top_repo_information">get_all_top_repo_information</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>