<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>top_github_scraper.utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>top_github_scraper.utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from dataclasses import dataclass
from bs4 import BeautifulSoup
import requests
from rich.progress import track
from rich import print 
import pandas as pd 
import os
import warnings
from dotenv import load_dotenv
from typing import List
from IPython import get_ipython
from tqdm import tqdm
import logging


load_dotenv()
warnings.filterwarnings(&#34;ignore&#34;)

TYPES = [&#39;Users&#39;, &#39;Repositories&#39;, &#39;Code&#39;, &#39;Commits&#39;, &#39;Issues&#39;, &#39;Packages&#39;, &#39;Topics&#39;]
SORT_BY = {&#39;Users&#39;: [&#39;followers&#39;],
            &#39;Repositories&#39;: [&#39;&#39;, &#39;stars&#39;]}
SCRAPE_CLASS = {&#39;Users&#39;: &#39;mr-1&#39;, &#39;Repositories&#39;: &#34;v-align-middle&#34;}

USERNAME = os.getenv(&#34;GITHUB_USERNAME&#34;)
TOKEN = os.getenv(&#34;GITHUB_TOKEN&#34;)

if USERNAME is None or TOKEN is None:
    logging.warning(&#34;&#34;&#34;You are using Github API as an unauthenticated user. For unauthenticated requests, the rate limit allows for up to 60 requests per hour.
     Follow the instruction here to be authenticated and increase your rate limit: https://github.com/khuyentran1401/top-github-scraper#setup&#34;&#34;&#34;)
class ScrapeGithubUrl:
    &#34;&#34;&#34;Scrape top Github urls based on a certain keyword and type

    Parameters
    -------
    keyword: str
        keyword to search on Github
    type: str
        whether to search for User or Repositories
    sort_by: str 
        sort by best match or most stars, by default &#39;best_match&#39;, which will sort by best match. 
        Use &#39;stars&#39; to sort by most stars.
    start_page_num: int
        page number to start scraping. The default is 0
    stop_page_num: int
        page number to stop scraping

    Returns
    -------
    List[str]
    &#34;&#34;&#34;

    def __init__(self,keyword: str, type: str, sort_by: str, start_page_num: int, stop_page_num: int):
        self.keyword = keyword
        self.type = type
        self.start_page_num = start_page_num
        self.stop_page_num = stop_page_num
        if sort_by ==&#39;best_match&#39;:
            self.sort_by = &#39;&#39;
        else:
            self.sort_by = sort_by

    @staticmethod
    def _keyword_to_url(page_num: int, keyword: str, type: str, sort_by: str):
        &#34;&#34;&#34;Change keyword to a url&#34;&#34;&#34;
        keyword_no_space = (&#34;+&#34;).join(keyword.split(&#34; &#34;))
        return f&#34;https://github.com/search?o=desc&amp;p={str(page_num)}&amp;q={keyword_no_space}&amp;s={sort_by}&amp;type={type}&#34;

    def _scrape_top_repo_url_one_page(self, page_num: int):
        &#34;&#34;&#34;Scrape urls of top Github repositories in 1 page&#34;&#34;&#34;
        url = self._keyword_to_url(page_num, self.keyword, type=self.type, sort_by=self.sort_by)
        page = requests.get(url)

        soup = BeautifulSoup(page.text, &#34;html.parser&#34;)
        a_tags = soup.find_all(&#34;a&#34;, class_=SCRAPE_CLASS[self.type])
        urls = [a_tag.get(&#34;href&#34;) for a_tag in a_tags]

        return urls

    def scrape_top_repo_url_multiple_pages(self):
        &#34;&#34;&#34;Scrape urls of top Github repositories in multiple pages&#34;&#34;&#34;
        urls = []
        if isnotebook():
            for page_num in tqdm(
                range(self.start_page_num, self.stop_page_num),
                desc=&#34;Scraping top GitHub URLs...&#34;,
            ):
                urls.extend(self._scrape_top_repo_url_one_page(page_num))
        else: 
            for page_num in track(
                range(self.start_page_num, self.stop_page_num),
                description=&#34;Scraping top GitHub URLs...&#34;,
            ):
                urls.extend(self._scrape_top_repo_url_one_page(page_num))

        return urls

class UserProfileGetter:
    &#34;&#34;&#34;Get the information from users&#39; homepage&#34;&#34;&#34;

    def __init__(self, urls: List[str]) -&gt; pd.DataFrame:
        self.urls = urls
        self.profile_features = [
            &#34;login&#34;,
            &#34;url&#34;,
            &#34;type&#34;,
            &#34;name&#34;,
            &#34;company&#34;,
            &#34;location&#34;,
            &#34;email&#34;,
            &#34;hireable&#34;,
            &#34;bio&#34;,
            &#34;public_repos&#34;,
            &#34;public_gists&#34;,
            &#34;followers&#34;,
            &#34;following&#34;,
        ]

    def _get_one_user_profile(self, profile_url: str):
        profile = requests.get(profile_url, auth=(USERNAME, TOKEN)).json()
        return {
            key: val
            for key, val in profile.items()
            if key in self.profile_features
        }

    def get_all_user_profiles(self):

        if isnotebook():
            all_contributors = [
            self._get_one_user_profile(url)
            for url in tqdm(
                self.urls, desc=&#34;Scraping top GitHub profiles...&#34;
            )
        ]
        else:
            all_contributors = [
                self._get_one_user_profile(url)
                for url in track(
                    self.urls, description=&#34;Scraping top GitHub profiles...&#34;
                )
            ]
        all_contributors_df = pd.DataFrame(all_contributors).reset_index(
            drop=True
        )

        return all_contributors_df


def isnotebook():
    try:
        shell = get_ipython().__class__.__name__
        if shell == &#39;ZMQInteractiveShell&#39;:
            return True   # Jupyter notebook or qtconsole
        elif shell == &#39;TerminalInteractiveShell&#39;:
            return False  # Terminal running IPython
        else:
            return False  # Other type (?)
    except NameError:
        return False      # Probably standard Python interpreter</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="top_github_scraper.utils.isnotebook"><code class="name flex">
<span>def <span class="ident">isnotebook</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def isnotebook():
    try:
        shell = get_ipython().__class__.__name__
        if shell == &#39;ZMQInteractiveShell&#39;:
            return True   # Jupyter notebook or qtconsole
        elif shell == &#39;TerminalInteractiveShell&#39;:
            return False  # Terminal running IPython
        else:
            return False  # Other type (?)
    except NameError:
        return False      # Probably standard Python interpreter</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="top_github_scraper.utils.ScrapeGithubUrl"><code class="flex name class">
<span>class <span class="ident">ScrapeGithubUrl</span></span>
<span>(</span><span>keyword: str, type: str, sort_by: str, start_page_num: int, stop_page_num: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Scrape top Github urls based on a certain keyword and type</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>keyword</code></strong> :&ensp;<code>str</code></dt>
<dd>keyword to search on Github</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>str</code></dt>
<dd>whether to search for User or Repositories</dd>
<dt><strong><code>sort_by</code></strong> :&ensp;<code>str </code></dt>
<dd>sort by best match or most stars, by default 'best_match', which will sort by best match.
Use 'stars' to sort by most stars.</dd>
<dt><strong><code>start_page_num</code></strong> :&ensp;<code>int</code></dt>
<dd>page number to start scraping. The default is 0</dd>
<dt><strong><code>stop_page_num</code></strong> :&ensp;<code>int</code></dt>
<dd>page number to stop scraping</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[str]</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapeGithubUrl:
    &#34;&#34;&#34;Scrape top Github urls based on a certain keyword and type

    Parameters
    -------
    keyword: str
        keyword to search on Github
    type: str
        whether to search for User or Repositories
    sort_by: str 
        sort by best match or most stars, by default &#39;best_match&#39;, which will sort by best match. 
        Use &#39;stars&#39; to sort by most stars.
    start_page_num: int
        page number to start scraping. The default is 0
    stop_page_num: int
        page number to stop scraping

    Returns
    -------
    List[str]
    &#34;&#34;&#34;

    def __init__(self,keyword: str, type: str, sort_by: str, start_page_num: int, stop_page_num: int):
        self.keyword = keyword
        self.type = type
        self.start_page_num = start_page_num
        self.stop_page_num = stop_page_num
        if sort_by ==&#39;best_match&#39;:
            self.sort_by = &#39;&#39;
        else:
            self.sort_by = sort_by

    @staticmethod
    def _keyword_to_url(page_num: int, keyword: str, type: str, sort_by: str):
        &#34;&#34;&#34;Change keyword to a url&#34;&#34;&#34;
        keyword_no_space = (&#34;+&#34;).join(keyword.split(&#34; &#34;))
        return f&#34;https://github.com/search?o=desc&amp;p={str(page_num)}&amp;q={keyword_no_space}&amp;s={sort_by}&amp;type={type}&#34;

    def _scrape_top_repo_url_one_page(self, page_num: int):
        &#34;&#34;&#34;Scrape urls of top Github repositories in 1 page&#34;&#34;&#34;
        url = self._keyword_to_url(page_num, self.keyword, type=self.type, sort_by=self.sort_by)
        page = requests.get(url)

        soup = BeautifulSoup(page.text, &#34;html.parser&#34;)
        a_tags = soup.find_all(&#34;a&#34;, class_=SCRAPE_CLASS[self.type])
        urls = [a_tag.get(&#34;href&#34;) for a_tag in a_tags]

        return urls

    def scrape_top_repo_url_multiple_pages(self):
        &#34;&#34;&#34;Scrape urls of top Github repositories in multiple pages&#34;&#34;&#34;
        urls = []
        if isnotebook():
            for page_num in tqdm(
                range(self.start_page_num, self.stop_page_num),
                desc=&#34;Scraping top GitHub URLs...&#34;,
            ):
                urls.extend(self._scrape_top_repo_url_one_page(page_num))
        else: 
            for page_num in track(
                range(self.start_page_num, self.stop_page_num),
                description=&#34;Scraping top GitHub URLs...&#34;,
            ):
                urls.extend(self._scrape_top_repo_url_one_page(page_num))

        return urls</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="top_github_scraper.utils.ScrapeGithubUrl.scrape_top_repo_url_multiple_pages"><code class="name flex">
<span>def <span class="ident">scrape_top_repo_url_multiple_pages</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Scrape urls of top Github repositories in multiple pages</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scrape_top_repo_url_multiple_pages(self):
    &#34;&#34;&#34;Scrape urls of top Github repositories in multiple pages&#34;&#34;&#34;
    urls = []
    if isnotebook():
        for page_num in tqdm(
            range(self.start_page_num, self.stop_page_num),
            desc=&#34;Scraping top GitHub URLs...&#34;,
        ):
            urls.extend(self._scrape_top_repo_url_one_page(page_num))
    else: 
        for page_num in track(
            range(self.start_page_num, self.stop_page_num),
            description=&#34;Scraping top GitHub URLs...&#34;,
        ):
            urls.extend(self._scrape_top_repo_url_one_page(page_num))

    return urls</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="top_github_scraper.utils.UserProfileGetter"><code class="flex name class">
<span>class <span class="ident">UserProfileGetter</span></span>
<span>(</span><span>urls: List[str])</span>
</code></dt>
<dd>
<div class="desc"><p>Get the information from users' homepage</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UserProfileGetter:
    &#34;&#34;&#34;Get the information from users&#39; homepage&#34;&#34;&#34;

    def __init__(self, urls: List[str]) -&gt; pd.DataFrame:
        self.urls = urls
        self.profile_features = [
            &#34;login&#34;,
            &#34;url&#34;,
            &#34;type&#34;,
            &#34;name&#34;,
            &#34;company&#34;,
            &#34;location&#34;,
            &#34;email&#34;,
            &#34;hireable&#34;,
            &#34;bio&#34;,
            &#34;public_repos&#34;,
            &#34;public_gists&#34;,
            &#34;followers&#34;,
            &#34;following&#34;,
        ]

    def _get_one_user_profile(self, profile_url: str):
        profile = requests.get(profile_url, auth=(USERNAME, TOKEN)).json()
        return {
            key: val
            for key, val in profile.items()
            if key in self.profile_features
        }

    def get_all_user_profiles(self):

        if isnotebook():
            all_contributors = [
            self._get_one_user_profile(url)
            for url in tqdm(
                self.urls, desc=&#34;Scraping top GitHub profiles...&#34;
            )
        ]
        else:
            all_contributors = [
                self._get_one_user_profile(url)
                for url in track(
                    self.urls, description=&#34;Scraping top GitHub profiles...&#34;
                )
            ]
        all_contributors_df = pd.DataFrame(all_contributors).reset_index(
            drop=True
        )

        return all_contributors_df</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="top_github_scraper.utils.UserProfileGetter.get_all_user_profiles"><code class="name flex">
<span>def <span class="ident">get_all_user_profiles</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_all_user_profiles(self):

    if isnotebook():
        all_contributors = [
        self._get_one_user_profile(url)
        for url in tqdm(
            self.urls, desc=&#34;Scraping top GitHub profiles...&#34;
        )
    ]
    else:
        all_contributors = [
            self._get_one_user_profile(url)
            for url in track(
                self.urls, description=&#34;Scraping top GitHub profiles...&#34;
            )
        ]
    all_contributors_df = pd.DataFrame(all_contributors).reset_index(
        drop=True
    )

    return all_contributors_df</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="top_github_scraper" href="index.html">top_github_scraper</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="top_github_scraper.utils.isnotebook" href="#top_github_scraper.utils.isnotebook">isnotebook</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="top_github_scraper.utils.ScrapeGithubUrl" href="#top_github_scraper.utils.ScrapeGithubUrl">ScrapeGithubUrl</a></code></h4>
<ul class="">
<li><code><a title="top_github_scraper.utils.ScrapeGithubUrl.scrape_top_repo_url_multiple_pages" href="#top_github_scraper.utils.ScrapeGithubUrl.scrape_top_repo_url_multiple_pages">scrape_top_repo_url_multiple_pages</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="top_github_scraper.utils.UserProfileGetter" href="#top_github_scraper.utils.UserProfileGetter">UserProfileGetter</a></code></h4>
<ul class="">
<li><code><a title="top_github_scraper.utils.UserProfileGetter.get_all_user_profiles" href="#top_github_scraper.utils.UserProfileGetter.get_all_user_profiles">get_all_user_profiles</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>